# The-Machine

A privacy-focused, modular pipeline for processing phone call audio and other recordings. Automates ingestion, PII removal, file tracking, audio separation, CLAP annotation, loudness normalization, speaker diarization, transcription, soundbite extraction, LLM integration, remixing, and show creation. All steps are orchestrated for strict privacy, traceability, and manifest/logging requirements.

---

**GitHub Repository:** [https://github.com/akspa0/The-Machine](https://github.com/akspa0/The-Machine)

---

## ⚠️ Environment & Installation (Conda Recommended)

> **The-Machine is a complex, GPU-accelerated pipeline with many dependencies. We strongly recommend using [Anaconda/conda](https://docs.conda.io/en/latest/) to manage your Python environment, especially for PyTorch and GPU support.**
>
> - Conda ensures correct versions of PyTorch, torchaudio, and CUDA for your hardware.
> - Pip-only installs are possible but not recommended for most users.

### 1. Create and Activate a Conda Environment
```sh
conda create -n themachine python=3.10
conda activate themachine
```

### 2. Install PyTorch (Choose the right CUDA version for your system)
See [PyTorch.org](https://pytorch.org/get-started/locally/) for the latest command.
- **CPU only:**
  ```sh
  conda install pytorch torchaudio cpuonly -c pytorch
  ```
- **CUDA 11.8 (NVIDIA GPU):**
  ```sh
  conda install pytorch torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
  ```

### 3. Install the rest of the dependencies
```sh
pip install -r requirements.txt
```

---

## 🧠 Local LLM Inference with LM Studio

The-Machine supports local LLM inference using [LM Studio](https://lmstudio.ai/), allowing you to run powerful language models on your own hardware for privacy and speed.

### 1. Install LM Studio
- Download LM Studio from [https://lmstudio.ai/](https://lmstudio.ai/).
- Available for Windows, Mac, and Linux.
- Follow the installation instructions for your platform.

### 2. Download a Recommended Model
- We recommend using a high-quality, chat-optimized model in GGUF format. For example:
  - **llama-3.1-8b-supernova-etherealhermes (Q4_K_M GGUF):** [Model Card & Download](https://huggingface.co/etherealhermes/llama-3.1-8b-supernova-etherealhermes-GGUF)
- Download the desired quantization (e.g., Q4_K_M for a good balance of speed and quality).

### 3. Load the Model in LM Studio
- Open LM Studio and use the 'Download Model' feature, or manually place the GGUF file in your models directory.
- In LM Studio, select the model and click 'Launch'.
- Ensure the local API server is enabled (default: `http://localhost:1234/v1`).
  - You can check this in the LM Studio settings under 'API Server'.

### 4. Configure The-Machine to Use LM Studio
- In your LLM config (e.g., `workflows/llm_tasks.json`), set the API endpoint to `http://localhost:1234/v1`.
- Or, use the CLI flag to specify the config:
  ```sh
  python pipeline_orchestrator.py <input_dir> --llm_config workflows/llm_tasks.json
  ```
- Make sure your LLM config matches the model's expected prompt format (e.g., chat template, system prompt).

### 5. Example LLM Config Snippet
```json
{
  "api_base": "http://localhost:1234/v1",
  "model": "llama-3.1-8b-supernova-etherealhermes",
  "temperature": 0.7,
  "max_tokens": 2048,
  "system_prompt": "You are a helpful assistant."
}
```

### 6. Troubleshooting
- **LM Studio not running:** Make sure LM Studio is open and the API server is enabled.
- **Port conflicts:** Ensure nothing else is using port 1234, or change the port in LM Studio and your config.
- **Model loading errors:** Verify you downloaded the correct GGUF file and quantization for your hardware.
- **API errors:** Test the endpoint in your browser: [http://localhost:1234/v1](http://localhost:1234/v1) should return a JSON response if running.

---

## Major Features & Recent Changes
- **Soundbites folders** are now named after sanitized call titles, not just call IDs.
- **SFW (safe-for-work) call titles** and show summaries are generated and included in outputs.
- **Show notes** are generated by the LLM and appended to show summaries.
- **Extensions system:** Users can add scripts (like `character_persona_builder.py`) to an `extensions/` folder, which run after the main pipeline and can use all outputs.
- **Advanced Character.AI persona generation:** The extension system supports per-channel and per-speaker persona creation, robust to folder naming and batch/single-file workflows.
- **CLAP segmentation and annotation** are now handled exclusively by the extension (`clap_segmentation_experiment.py`) in the `extensions/` folder. The main pipeline is fully decoupled from CLAP.
- **All outputs, logs, and manifests** are strictly PII-free and fully auditable.
- **Timestamps** in show summaries are now in HH:MM:SS format, with emoji for calls/tones.
- **Resume and force:** The pipeline is robust to resume, with `--resume` and `--resume-from`, and `--force` now archives outputs instead of deleting them.

---

## Features
- **Privacy-first:** No PII in filenames, logs, or outputs. All logging/manifesting is anonymized.
- **Modular pipeline:** Ingestion, separation, CLAP, diarization, normalization, transcription, soundbite, remix, show, LLM, and more.
- **CLAP-based segmentation:** (Extension only) Detects call boundaries in long audio using CLAP via the `clap_segmentation_experiment.py` extension (configurable prompts, thresholds).
- **Speaker diarization:** Segments audio by speaker, with per-speaker transcripts and outputs.
- **LLM integration:** Workflow-driven LLM tasks (titles, synopses, categories, image prompts, songs, etc.)
- **Batch processing:** Handles large folders of audio, tuples, or single files (including YouTube/URL inputs).
- **Extensible:** All workflows and prompts are JSON-configurable in the `workflows/` folder.
- **Traceability:** Full manifest and metadata lineage for every file and output.

---

## Directory Structure
```
The-Machine/
  memory-bank/           # Project docs, context, and rules
  outputs/               # All run outputs (timestamped folders)
  workflows/             # All pipeline, CLAP, and LLM configs (JSON)
  extensions/            # Custom extension scripts (see below)
  ...                    # Pipeline scripts and modules
```

---

## Extensions

You can add custom scripts to the `extensions/` folder. These scripts will be run after the main pipeline completes and can use all outputs (soundbites, transcripts, LLM results, etc.).

### Character Persona Builder Extension

- The `character_persona_builder.py` extension generates advanced Character.AI persona definitions for each call/channel/speaker using transcripts and LLMs.
- **Channel folders may have run-specific prefixes** (e.g., `0000-conversation`). The extension normalizes these for output and is robust to naming.
- **For conversation-only calls:** Generates a separate persona for each detected speaker (no merging).
- **For left/right calls:** Merges all speakers per channel and generates one persona per channel.
- **System prompt and persona style are embedded** for best results (no external files needed).
- **Usage example:**
  ```sh
  python extensions/character_persona_builder.py outputs/run-YYYYMMDD-HHMMSS --llm-config workflows/llm_tasks.json
  ```
- Outputs are written to `characters/<call_title or call_id>/<channel or conversation_speaker>/` with transcript, persona, and audio clips.

### General Extension Best Practices
- Extensions should be robust to folder naming and support both batch and single-file workflows.
- Log only anonymized, PII-free information.
- See `extensions/README.md` for more details and extension authoring tips.

---

## Configuration
- **CLAP segmentation/annotation:** Now handled exclusively by the extension (`clap_segmentation_experiment.py`). You may still configure prompts and thresholds in `workflows/clap_segmentation.json` and `workflows/clap_annotation.json` for use by the extension.
- **LLM tasks:** `