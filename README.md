# The-Machine

---

Dedicated to Carlito Cross - RIP - [MadhouseLive](https://www.madhouselive.com/)

---

A privacy-focused, modular pipeline for processing phone call audio and other recordings. Automates ingestion, PII removal, file tracking, audio separation, CLAP annotation, loudness normalization, speaker diarization, transcription, soundbite extraction, LLM integration, remixing, and show creation. All steps are orchestrated for strict privacy, traceability, and manifest/logging requirements.

---

**GitHub Repository:** [https://github.com/akspa0/The-Machine](https://github.com/akspa0/The-Machine)

---

## ⚠️ Environment & Installation (Conda Recommended)

> **The-Machine is a complex, GPU-accelerated pipeline with many dependencies. We strongly recommend using [Anaconda/conda](https://docs.conda.io/en/latest/) to manage your Python environment, especially for PyTorch and GPU support.**
>
> - Conda ensures correct versions of PyTorch, torchaudio, and CUDA for your hardware.
> - Pip-only installs are possible but not recommended for most users.

### 1. Create and Activate a Conda Environment
```sh
conda create -n themachine python=3.10
conda activate themachine
```

### 2. Install PyTorch (Choose the right CUDA version for your system)
See [PyTorch.org](https://pytorch.org/get-started/locally/) for the latest command.
- **CPU only:**
  ```sh
  conda install pytorch torchaudio cpuonly -c pytorch
  ```
- **CUDA 11.8 (NVIDIA GPU):**
  ```sh
  conda install pytorch torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
  ```

### 3. Install the rest of the dependencies
```sh
pip install -r requirements.txt
```

---

## Major Features & Recent Changes
- **Soundbites folders** are now named after sanitized call titles, not just call IDs.
- **SFW (safe-for-work) call titles** and show summaries are generated and included in outputs.
- **Show notes** are generated by the LLM and appended to show summaries.
- **Extensions system:** Users can add scripts (like `character_persona_builder.py`) to an `extensions/` folder, which run after the main pipeline and can use all outputs.
- **Advanced Character.AI persona generation:** The extension system supports per-channel and per-speaker persona creation, robust to folder naming and batch/single-file workflows.
- **CLAP segmentation and annotation** are now handled exclusively by the extension (`clap_segmentation_experiment.py`) in the `extensions/` folder. The main pipeline is fully decoupled from CLAP.
- **All outputs, logs, and manifests** are strictly PII-free and fully auditable.
- **Timestamps** in show summaries are now in HH:MM:SS format, with emoji for calls/tones.
- **Resume and force:** The pipeline is robust to resume, with `--resume` and `--resume-from`, and `--force` now archives outputs instead of deleting them.

---

## Features
- **Privacy-first:** No PII in filenames, logs, or outputs. All logging/manifesting is anonymized.
- **Modular pipeline:** Ingestion, separation, CLAP, diarization, normalization, transcription, soundbite, remix, show, LLM, and more.
- **CLAP-based segmentation:** (Extension only) Detects call boundaries in long audio using CLAP via the `clap_segmentation_experiment.py` extension (configurable prompts, thresholds).
- **Speaker diarization:** Segments audio by speaker, with per-speaker transcripts and outputs.
- **LLM integration:** Workflow-driven LLM tasks (titles, synopses, categories, image prompts, songs, etc.)
- **Batch processing:** Handles large folders of audio, tuples, or single files (including YouTube/URL inputs).
- **Extensible:** All workflows and prompts are JSON-configurable in the `workflows/` folder.
- **Traceability:** Full manifest and metadata lineage for every file and output.

---

## Directory Structure
```
The-Machine/
  memory-bank/           # Project docs, context, and rules
  outputs/               # All run outputs (timestamped folders)
  workflows/             # All pipeline, CLAP, and LLM configs (JSON)
  extensions/            # Custom extension scripts (see below)
  ...                    # Pipeline scripts and modules
```

---

## Extensions

You can add custom scripts to the `extensions/` folder. These scripts will be run after the main pipeline completes and can use all outputs (soundbites, transcripts, LLM results, etc.).

### Character Persona Builder Extension

- The `character_persona_builder.py` extension generates advanced Character.AI persona definitions for each call/channel/speaker using transcripts and LLMs.
- **Channel folders may have run-specific prefixes** (e.g., `0000-conversation`). The extension normalizes these for output and is robust to naming.
- **For conversation-only calls:** Generates a separate persona for each detected speaker (no merging).
- **For left/right calls:** Merges all speakers per channel and generates one persona per channel.
- **System prompt and persona style are embedded** for best results (no external files needed).
- **Usage example:**
  ```sh
  python extensions/character_persona_builder.py outputs/run-YYYYMMDD-HHMMSS --llm-config workflows/llm_tasks.json
  ```
- Outputs are written to `characters/<call_title or call_id>/<channel or conversation_speaker>/` with transcript, persona, and audio clips.

### Avatar SDXL Generator Extension

- The `avatar_sdxl_generator.py` extension (in `extensions/avatar/`) generates persona (avatar) and backdrop images for each call/persona using SDXL workflows.
- **Output structure:**
  - `comfyui_images/avatar/backdrops/` — Backdrop images for each call.
  - `comfyui_images/avatar/{call_id}/{speaker}/` — Persona images for each speaker in each call.
  - `comfyui_images/avatar/image_manifest.json` — Manifest mapping call_id to backdrop and (call_id, speaker) to persona image.
- **Workflow customization:** Uses separate, tweakable SDXL workflow JSONs for avatars and backdrops (`avatar_sdxl_workflow.json`, `backdrop_sdxl_workflow.json`).
- **CLI options:**
  - `--initial-prompt` to control style (e.g., "a drawing of", "a photograph of").
  - `--avatar-workflow` and `--backdrop-workflow` to specify custom workflows.
- **Usage example:**
  ```sh
  python extensions/avatar/sdxl_avatar_generator.py \
    --persona-manifest outputs/run-YYYYMMDD-HHMMSS/characters/persona_manifest.json \
    --output-root outputs/run-YYYYMMDD-HHMMSS \
    --initial-prompt "a drawing of"
  ```
- Outputs are ready for downstream animation, lipsync, and compositing steps.

## General Extension Best Practices
- Extensions should be robust to folder naming and support both batch and single-file workflows.
- Log only anonymized, PII-free information.
- See `extensions/README.md` for more details and extension authoring tips.

### ComfyUI Image/Video Generator Extension

The ComfyUI extension (`extensions/comfyui_image_generator.py`) enables LLM-driven image and video generation from audio transcripts. It is fully integrated with the pipeline and supports robust, privacy-focused, and context-rich prompt generation for downstream ComfyUI workflows.

### Key Features
- **Token-aware, context-preserving segmentation:**
  - Transcripts are segmented into windows of 90 seconds (default, configurable via `--window-seconds`).
  - Each segment is checked to ensure it does not exceed the token limit (default 4096, configurable up to 16384, hard cap 23000).
  - If a segment would exceed the token limit, it is split at the last sentence/utterance boundary before the limit to preserve context.
  - This prevents excessive splitting (e.g., 99 jobs for a 54-minute file) and ensures each prompt is context-rich and LLM-friendly.
- **CLI options:**
  - `--window-seconds <N>`: Time window size for scene segmentation (default: 90, recommended: 60-120)
  - `--max-tokens <N>`: Maximum number of tokens per segment (default: 4096, max: 16384, hard cap: 23000)
  - `--force`: Force regeneration of all prompts and scene prompt JSONs, even if they already exist.
- **Consolidated batching:**
  - The number of LLM jobs is now proportional to audio length and window size, not utterance count. For example, a 54-minute file with 90s windows yields about 36 jobs.
- **Privacy and traceability:**
  - All prompts and outputs are anonymized and PII-free, with full manifest tracking.

### Usage Example
```sh
python extensions/comfyui_image_generator.py \
  --run-folder outputs/run-YYYYMMDD-HHMMSS \
  --workflow extensions/ComfyUI/theMachine_SDXL_Basic.json \
  --image --window-seconds 90 --max-tokens 4096
```

See `extensions/comfyui_image_generator_README.md` for advanced options, troubleshooting, and integration details.

---

## Configuration
- **CLAP segmentation/annotation:** Now handled exclusively by the extension (`clap_segmentation_experiment.py`). You may still configure prompts and thresholds in `workflows/clap_segmentation.json` and `workflows/clap_annotation.json` for use by the extension.
- **LLM tasks:** `workflows/llm_tasks.json` (task list, prompts, model config)
- **All configs are editable JSON files.**

---

## Usage

### Basic CLI Example
```sh
python pipeline_orchestrator.py <input_dir>
```

### With CLAP-based Call Segmentation (Extension Only)
```sh
python extensions/clap_segmentation_experiment.py outputs/run-YYYYMMDD-HHMMSS --config workflows/clap_segmentation.json
```

### Processing a YouTube/URL Input
```sh
python pipeline_orchestrator.py --url "https://www.youtube.com/watch?v=..."
```

### Resume/Debug
```sh
python pipeline_orchestrator.py --output-folder outputs/run-YYYYMMDD-HHMMSS --resume
```

### Other CLI Options
- `--asr_engine parakeet|whisper` (choose ASR model)
- `--llm_config workflows/llm_tasks.json` (custom LLM task config)
- `--call-tones` (insert tones between calls in show output)
- `--resume-from <stage>` (resume from a specific stage)

---

## Customization
- **CLAP/LLM prompts, thresholds, and logic** are fully tweakable in the `workflows/` JSON files.
- Change prompts, add/remove tasks, adjust thresholds, and rerun the pipeline—no code changes needed.

---

## Troubleshooting
- **PyTorch install issues:** Use conda and follow the [official instructions](https://pytorch.org/get-started/locally/).
- **No segments detected:** Lower the CLAP confidence threshold or add more prompts in `clap_segmentation.json`.
- **Too many/false segments:** Raise the threshold or adjust pairing/gap settings.
- **LLM token limit errors:** The pipeline will chunk transcripts by speaker/segment automatically.
- **Manifest/logs:** Check the output run folder for detailed logs and manifest.json.
- **If prompts or scene prompt JSONs do not update after changing window or token settings, use the `--force` option to force regeneration. Otherwise, the script will reuse existing files.**

---

## Contributing & Support
- PRs and issues welcome!
- For questions, open an issue or contact the maintainer.

---

**Happy hacking with The-Machine!** 